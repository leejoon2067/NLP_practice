{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 웹페이지 뉴스(네이버) 기반 Q & A 챗봇 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\leejo\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG TUTORIAL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.1.17-py3-none-any.whl (867 kB)\n",
      "     -------------------------------------- 867.6/867.6 kB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from langchain) (3.9.3)\n",
      "Collecting pydantic<3,>=1\n",
      "  Downloading pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "     -------------------------------------- 409.3/409.3 kB 6.4 MB/s eta 0:00:00\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.5-py3-none-any.whl (28 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.36\n",
      "  Downloading langchain_community-0.0.36-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 9.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from langchain) (1.25.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Collecting langsmith<0.2.0,>=0.1.17\n",
      "  Downloading langsmith-0.1.54-py3-none-any.whl (116 kB)\n",
      "     -------------------------------------- 116.7/116.7 kB 3.4 MB/s eta 0:00:00\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.48\n",
      "  Downloading langchain_core-0.1.50-py3-none-any.whl (302 kB)\n",
      "     -------------------------------------- 302.8/302.8 kB 9.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
      "     ---------------------------------------- 49.3/49.3 kB ? eta 0:00:00\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
      "Collecting packaging<24.0,>=23.2\n",
      "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "     ---------------------------------------- 53.0/53.0 kB ? eta 0:00:00\n",
      "Collecting orjson<4.0.0,>=3.9.14\n",
      "  Downloading orjson-3.10.3-cp310-none-win_amd64.whl (138 kB)\n",
      "     -------------------------------------- 138.8/138.8 kB 4.0 MB/s eta 0:00:00\n",
      "Collecting typing-extensions>=4.6.1\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Collecting pydantic-core==2.18.2\n",
      "  Downloading pydantic_core-2.18.2-cp310-none-win_amd64.whl (1.9 MB)\n",
      "     ---------------------------------------- 1.9/1.9 MB 5.8 MB/s eta 0:00:00\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\n",
      "Installing collected packages: typing-extensions, packaging, orjson, jsonpatch, annotated-types, typing-inspect, pydantic-core, marshmallow, pydantic, dataclasses-json, langsmith, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 22.0\n",
      "    Uninstalling packaging-22.0:\n",
      "      Successfully uninstalled packaging-22.0\n",
      "  Attempting uninstall: jsonpatch\n",
      "    Found existing installation: jsonpatch 1.32\n",
      "    Uninstalling jsonpatch-1.32:\n",
      "      Successfully uninstalled jsonpatch-1.32\n",
      "Successfully installed annotated-types-0.6.0 dataclasses-json-0.6.5 jsonpatch-1.33 langchain-0.1.17 langchain-community-0.0.36 langchain-core-0.1.50 langchain-text-splitters-0.0.1 langsmith-0.1.54 marshmallow-3.21.2 orjson-3.10.3 packaging-23.2 pydantic-2.7.1 pydantic-core-2.18.2 typing-extensions-4.11.0 typing-inspect-0.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ratsnlp 1.0.53 requires transformers==4.28.1, but you have transformers 4.39.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-0.1.6-py3-none-any.whl (34 kB)\n",
      "Collecting tiktoken<1,>=0.5.2\n",
      "  Downloading tiktoken-0.6.0-cp310-cp310-win_amd64.whl (798 kB)\n",
      "     -------------------------------------- 798.7/798.7 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting openai<2.0.0,>=1.24.0\n",
      "  Downloading openai-1.25.2-py3-none-any.whl (312 kB)\n",
      "     -------------------------------------- 312.9/312.9 kB 2.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.46 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from langchain_openai) (0.1.50)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (6.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (8.2.3)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (2.7.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (0.1.54)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.11.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (3.5.0)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "     ---------------------------------------- 75.6/75.6 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sniffio in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.64.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain_openai) (3.4)\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 77.9/77.9 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai) (2022.12.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.46->langchain_openai) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langchain_openai) (3.10.3)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langchain_openai) (2.18.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langchain_openai) (0.6.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain_openai) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain_openai) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.24.0->langchain_openai) (0.4.6)\n",
      "Installing collected packages: httpcore, distro, tiktoken, httpx, openai, langchain_openai\n",
      "Successfully installed distro-1.9.0 httpcore-1.0.5 httpx-0.27.0 langchain_openai-0.1.6 openai-1.25.2 tiktoken-0.6.0\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 데이터 가져오기 (웹 크롤링)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### bs4.SoupStrainer -> 웹에서 원하는 요소를 쉽게 가져오는 툴\n",
    "ex. bs4.SoupStrainer(\n",
    "    \"div\",\n",
    "    attrs = {\"class\": ['newsct_article _article_body', 'media_end_head_title']},\n",
    ")\n",
    "- 네이버 뉴스 참고.\"https://n.news.naver.com/mnews/article/025/0003358538\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n日 \"7월까지 답하라\" 초강수…\\'네이버 압박\\' 뒷배경 알고 보니\\n\\n\\n\\t\\t\\t  일본 정부가 메신저 애플리케이션 ‘라인’과 관련해 사실상의 탈(脫) 네이버 압력을 행사하는 배경엔 ‘라인야후’에 대한 의구심이 있다는 일본 언론 보도가 나왔다. 약 52만건에 달하는 개인정보 유출 사태가 네이버에 대한 과한 의존에서 비롯됐는데도 라인야후가 뚜렷한 개선책 마련 없이 일본 정부가 정한 시한인 7월마저 연기하려는 의도가 있다고 판단했다는 것이다.      일본 니혼게이자이신문(닛케이)은 6일 라인야후에 대한 일본 총무성의 두 차례 행정지도(3월5일, 4월16일)에 대해 “이례적”이라고 언급하며 배경으로 라인야후의 대책 마련 지연 의혹을 제기했다.      ━   총무성 “구체적 범위, 시기 기재 없어”      \\n\\n\\n\\n2019년 현재 지배구조로 전환한 라인야후. 교도=연합뉴스           이와 관련, 일본에서 약 9700만명이 사용해 ‘국민 메신저’로 불리는 라인 서비스에서 지난해 11월 개인정보 유출 사고가 발생했다. 네이버 클라우드가 사이버 공격을 받아 악성코드에 감염됐고, 이로 인해 일부 내부 시스템을 공유하던 라인야후의 개인정보 약 52만건이 유출됐다.        일본 총무성은 지난 3월 라인야후에 1차 행정지도를 내렸다. 라인야후 모회사 지분을 네이버와 일본 소프트뱅크가 50%씩 나눠갖고 있는 가운데 네이버가 100% 지분을 갖고 있는 \\'네이버 클라우드\\'를 이용하는 등 네이버에 대한 의존도가 지나치게 높은 게 유출 사고로 이어졌다는 문제의식이었다. 총무성은 이런 위탁관계를 어떻게 끊을 것인지를 핵심 논점으로 봤다. 이런 맥락에서 ‘탈 네이버’로 귀결되는 자본 관계 재검토가 문제가 불거졌다.      닛케이는 이후 총무성이 불과 한 달여만에 다시 행정지도를 내린 배경에는 라인야후가 4월1일 제출한 보고서가 있다고 지적했다. 라인야후측이 제출한 보고서에서 ‘2026년 12월까지 시스템을 분리’하고 지분관계에 대해서도 ‘관계 각사에 자본관계 재검토를 요청’한다고 기재했다는 것이다.       닛케이는 이와 관련해 “구체적 범위 및 시기 기재가 거의 적혀있지 않았다”는 총무성 담당자의 발언을 전했다. “라인야후가 총무성에 두꺼운 서류를 제출했지만 구체적인 내용이 없었다”는 것이다. 그러면서 닛케이는 “총무성도 행정지도로부터 불과 1개월 지난 시점인 4월 제출된 (라인야후의) 보고서에선 다소 불충분한 점이 있을 것으로 각오하고 있었다”는 설명을 보탰다. “라인야후가 큰 의사결정을 내릴 수 있는 것은 봄”이라는 전망에 따라 “총무성은 7월 보고서가 중심이 될 것이라고 봤다”는 것이다.       이처럼 총무성은 애초에 4월에 라인야후가 총무성에 제출하는 대책보고서가 미흡할 것을 감안하고 봤는데도, 실제 보고서가 수준 이하였다는 의미다.       이에 총무성 측은 “(라인야후가) 7월도 연기를 노리고 있다”고 의심했다고 닛케이는 전했다. 보고서를 보니 7월에도 만족할만한 대책안이 나올 가능성이 작다는 판단에 곧바로 2차 행정지도에 나섰다는 의미다. 실제 마쓰모토 다케아키(松本剛明) 총무상은 2차 행정지도 직후 “그룹 전체 보안 거버넌스 체제 구축에 대해 충분한 재검토가 이뤄질 전망이 명확하지 않다고 판단했다”고 말했다.       한편 네이버가 만든 라인 서비스는 2011년에 일본에서 선보였다. 동일본 대지진을 거치며 일본 국민 대다수가 사용하는 메신저 서비스로 자리 잡았다. 한·일 관계 부침 속에서도 성장한 라인은 2019년 야후 재팬을 자회사로 두고 있는 소프트뱅크와 네이버가 각각 50%씩 출자해 현재의 라인야후를 운영하는 ‘A홀딩스’를 세워 사업을 진행해왔다. 하지만 이번 총무성의 행정지도에 따라 라인야후와 네이버간의 지배 구조 재검토가 이뤄지면 라인야후의 경영권은 소프트뱅크 측으로 넘어갈 수 있다.   \\n\\n', metadata={'source': 'https://n.news.naver.com/mnews/article/025/0003358538'})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 뉴스 기사 내용 로드, 청크 나누고, 인덱싱까지\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths = (\"https://n.news.naver.com/mnews/article/025/0003358538\",),\n",
    "    bs_kwargs = dict(\n",
    "        parse_only = bs4.SoupStrainer(\n",
    "            \"div\",\n",
    "            attrs = {\"class\": ['newsct_article _article_body', 'media_end_head_title']},\n",
    "        )\n",
    "    )        \n",
    ")\n",
    "docs = loader.load()\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 데이터 split 하기\n",
    "LLM이 허용할 수 있는 단위의 크기로 데이터를 분할하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000, chunk_overlap =50)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAISS 혹은 Chroma 와 같은 vectorstore는 나뉘어진 청크를 바탕으로 문서의 벡터 표현을 생성함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore 생성. -> DB에 뉴스 기사를 저장. \n",
    "# FAISS -> vectorstore. OpenAIEmbeddings : 1536차원의 벡터 데이터\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents = splits, embedding = OpenAIEmbeddings(api_key='Your API_TOKEN'))\n",
    "\n",
    "# 뉴스에 포함된 정보를 검색하고 생성.\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchainhub\n",
      "  Downloading langchainhub-0.1.15-py3-none-any.whl (4.6 kB)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2\n",
      "  Downloading types_requests-2.31.0.20240406-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from langchainhub) (2.31.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchainhub) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchainhub) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchainhub) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\leejo\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchainhub) (1.26.14)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "     -------------------------------------- 121.1/121.1 kB 2.4 MB/s eta 0:00:00\n",
      "Installing collected packages: urllib3, types-requests, langchainhub\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.14\n",
      "    Uninstalling urllib3-1.26.14:\n",
      "      Successfully uninstalled urllib3-1.26.14\n",
      "Successfully installed langchainhub-0.1.15 types-requests-2.31.0.20240406 urllib3-2.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.41 requires requests_mock, which is not installed.\n",
      "pyppeteer 1.0.2 requires urllib3<2.0.0,>=1.25.8, but you have urllib3 2.2.1 which is incompatible.\n",
      "google-auth 2.22.0 requires urllib3<2.0, but you have urllib3 2.2.1 which is incompatible.\n",
      "conda-repo-cli 1.0.41 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.41 requires nbformat==5.4.0, but you have nbformat 5.7.0 which is incompatible.\n",
      "conda-repo-cli 1.0.41 requires requests==2.28.1, but you have requests 2.31.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchainhub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. vectorstore DB에서 원하는 데이터 검색하는 검색기(as_retriever 사용해서) 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorstore.as_retriever()를 통해 생성된 검색기는 hub.pull로 가져온 프롬프트와 ChatOpenAI 모델을 사용해 새로운 내용을 생성함. \n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature = 0, api_key = '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')\n",
    "# gpt-4-turbo-preview\n",
    "\n",
    "# 단락 별 주제 뽑아내고\n",
    "def format_docs(docs):\n",
    "    # 검색한 문서 결과를 하나의 문단으로 합쳐준다. \n",
    "    return '\\n\\n'.join(doc.page_content for doc in docs)\n",
    "\n",
    "# 체인을 생성함.\n",
    "rag_chain = (\n",
    "    {\"context\" : retriever | format_docs, \"question\" : RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'네이버 압박의 배경은 라인야후의 개인정보 유출 사태로 인한 네이버 의존도 증가와 라인야후의 대책 부재로 인한 일본 정부의 압박이 연기되려는 의도에 있다. 라인야후의 지배 구조 재검토로 소프트뱅크 측이 경영권을 획득할 가능성이 제기되고, 네이버 클라우드를 통한 의존도 문제가 중요한 논점으로 다뤄졌다.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\n",
    "    \"일본의 네이버 라인 압박에 대한 그 배경을 상세하게 설명해줘\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 라인야후의 개인정보 유출 사태에 대한 대책 마련 지연 의혹 제기\n",
      "- 네이버에 대한 의존도가 높아서 발생한 문제의식\n",
      "- 라인야후의 지배 구조 재검토와 네이버와의 자본 관계 재검토 필요\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    rag_chain.invoke(\"정부의 라인 압박에 대한 대책을 bullet points 형식으로 작성해줘\")\n",
    ") # 문서에 대한 질의를 입력하고, 답변을 출력한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'네이버 라인의 임직원 숫자는 알 수 없습니다.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\n",
    "    \"네이버 라인의 임직원 숫자는 몇 명인가?\"\n",
    ") # 프롬프트에는 hallucination(거짓 정보 제공)을 막기 위한 기능이 내재됨. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
